{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUGbt+ry4EsbBqmpwMtp07",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maheshkumar145/DL_Theory/blob/main/DL_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.\tDescribe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?**\n",
        "\n",
        "**Ans:** **ANN** is made of three layers namely input layer, output layer, and hidden layer/s. There must be a connection from the nodes in the input layer with the nodes in the hidden layer and from each hidden layer node with the nodes of the output layer. The input layer takes the data from the network.\n",
        "\n",
        "**ANN** is built as the structure of **BNN**, it basically has similar look as BNN. They are both the composition of neurons .BNN is consist of neurons as cells well ANN is consist of neurons as nodes and edges."
      ],
      "metadata": {
        "id": "GqLUJd9rS7Cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.What are the different types of activation functions popularly used? Explain each of them?**\n",
        "\n",
        "**Ans:**The Activation Functions are of 2 types\n",
        "             \n",
        "**Linear Activation Function :**\n",
        " \n",
        "It is a simple straight line activation function where our function is directly proportional to the weighted sum of neurons or input. Linear activation functions are better in giving a wide range of activations and a line of a positive slope may increase the firing rate as the input rate increases.\n",
        "\n",
        "**Non-linear Activation Functions:**\n",
        "\n",
        "*1.ReLU Activation function:*\n",
        " \n",
        "Rectified linear unit or ReLU is most widely used activation function right now which ranges from 0 to infinity.\n",
        "\n",
        "*2.Sigmoid Activation Function:*\n",
        " \n",
        "The sigmoid activation function is used mostly as it does its task with great efficiency, it basically is a probabilistic approach towards decision making and ranges in between 0 to 1, so when we have to make a decision or to predict an output we use this activation function because of the range is the minimum, therefore, prediction would be more accurate.\n",
        "\n",
        "*3.Hyperbolic Tangent Activation Function(Tanh):*\n",
        "\n",
        "This activation function is slightly better than the sigmoid function, like the sigmoid function it is also used to predict or to differentiate between two classes but it maps the negative input into negative quantity only and ranges in between -1 to  1."
      ],
      "metadata": {
        "id": "fOqnwJymS5Dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Explain, in details, Rosenblatt’s perceptron model.How can a set of data be classified using a simple perceptron?**\n",
        "\n",
        "**Ans:**Rosenblatt perceptron is a binary single neuron model. The inputs integration is implemented through the addition of the weighted inputs that have fixed weights obtained during the training stage. If the result of this addition is larger than a given threshold θ the neuron fires. When the neuron fires its output is set to 1, otherwise it’s set to 0.\n"
      ],
      "metadata": {
        "id": "bGzUFN9tS3Zq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.\tExplain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem.**\n",
        "\n",
        "**Ans:** Multi layer percepton is a neural network architecture with an input layer, hidden layer, and output layer. \n",
        "\n",
        "The XOR problem can be solved by using Multi-Layer Perceptrons because it contain multi layer.So during the forward propagation through the neural networks, the weights get updated to the corresponding layers and the XOR logic gets executed."
      ],
      "metadata": {
        "id": "aOYqLmeDS1j5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.What is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN?**\n",
        "\n",
        "**Ans:** Artificial neural networks are extremely powerful computational devices.ANNs are modeled on the basis of current brain theories, in which information is represented by weights.ANNs have massive parallelism which makes them very efficient.\n",
        "\n",
        " \n",
        "\n",
        "**(a)**    They can learn and generalize from training data so there is no need for enormous feats of programming.\n",
        "\n",
        " \n",
        "\n",
        "**(b)**    Storage is fault tolerant i.e. some portions of the neural net can be removed and there will be only a small degradation in the quality of stored data.\n",
        "\n",
        " \n",
        "\n",
        "**(c)**   They are particularly fault tolerant which is equivalent to the “graceful degradation” found in biological systems.\n",
        "\n",
        " \n",
        "\n",
        "**(d)**    Data are naturally stored in the form of associative memory which contrasts with conventional memory, in which data are recalled by specifying address of that data.\n",
        "\n",
        " \n",
        "\n",
        "**(e)**    They are very noise tolerant, so they can cope with situations where normal symbolic systems would have difficulty.\n",
        "\n",
        " \n",
        "\n",
        "**(f)**   In practice, they can do anything a symbolic/ logic system can do and more.\n",
        "\n",
        " \n",
        "\n",
        "**(g)**        Neural networks can extrapolate and intrapolate from their stored information. The neural networks can also be trained. Special training teaches the net to look for significant features or relationships of data."
      ],
      "metadata": {
        "id": "5ikzrKjwSztD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.\tExplain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?**\n",
        "\n",
        "**Ans:**Learning, in artificial neural network, is the method of modifying the weights of connections between the neurons of a specified network. Learning in ANN can be classified into three categories namely supervised learning, unsupervised learning, and reinforcement learning.\n",
        "\n",
        "Major challenge is issuing weigths and changing weigths while back and front propogation which has to be updated again and again until the error is zero."
      ],
      "metadata": {
        "id": "D5fotTGSSxqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.Explain, in details, the backpropagation algorithm. What are the limitations of this algorithm?**\n",
        "\n",
        "**Ans:**Backpropagation is an algorithm that is designed to test for errors working back from output nodes to input nodes.\n",
        "\n",
        "Limitations of the Backpropagation algorithm is slow, all previous layers are locked until gradients for the current layer is calculated. It suffers from vanishing or exploding gradients problem. It suffers from overfitting & underfitting problem."
      ],
      "metadata": {
        "id": "lr44hAawSunJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.Describe, in details, the process of adjusting the interconnection weights in a multi-layer neural network?**\n",
        "\n",
        "**Ans:** In order for a neural networks to learn, weights associated with neuron connections must be updated after forward passes of data through the network. These weights are adjusted to help reconcile the differences between the actual and predicted outcomes for subsequent forward passes.\n",
        "\n",
        "Difference between actual and predicted values, the error would be a useful measure here, and so each neuron will require that their respective error be sent backward through the network to them in order to facilitate the update process; hence, backpropagation of error. Updates to the neuron weights will be reflective of the magnitude of error propagated backward after a forward pass has been completed.\n"
      ],
      "metadata": {
        "id": "5crLpza9SseB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.\tWhat are the steps in the backpropagation algorithm? Why a multi-layer neural network is required?**\n",
        "\n",
        "**Ans:**  Forward Propagation -> Backward Propagation ->Putting all the values together and calculating the updated weight value.\n",
        "\n",
        "More hidden layers per layer you add more parameters to the model. Hence you allow the model to fit more complex functions."
      ],
      "metadata": {
        "id": "csn3dd4MSpAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.\tWrite short notes on:**\n",
        "\n",
        "**1.\tArtificial neuron :**\n",
        "An artificial neural network is an interconnected group of nodes, inspired by a simplification of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another.\n",
        "\n",
        "**2.\tMulti-layer perceptron :**\n",
        "Multi layer percepton is a neural network architecture with an input layer, hidden layer, and output layer.MLPs are useful in research for their ability to solve problems stochastically, which often allows approximate solutions for extremely complex problems like fitness approximation\n",
        "\n",
        "**3.\tDeep learning :**\n",
        "Deep learning is a subset of machine learning, which is essentially a neural network with three or more layers.Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign, or to distinguish a pedestrian from a lamppost.\n",
        "\n",
        "**4.\tLearning rate :**\n",
        "Learning rate is a hyper-parameter that defines the adjustment in the weights of our network with respect to the loss gradient descent. It determines how fast or slow we will move towards the optimal weights."
      ],
      "metadata": {
        "id": "Bteuw6ZDSn5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.\tWrite the difference between**\n",
        "\n",
        "**1.\tActivation function vs threshold function:**\n",
        "A threshold value determines whether a neuron should be activated or not activated in a binary step activation function. \n",
        "\n",
        "The activation function compares the input value to a threshold value. If the input value is greater than the threshold value, the neuron is activated.\n",
        "\n",
        "**2.\tStep function vs sigmoid function:**\n",
        "The step function is typically only useful within single-layer perceptrons, an early type of neural networks that can be used for classification in cases where the input data is linearly separable.\n",
        "\n",
        "Sigmoids can be useful when building more biologically realistic networks by introducing noise or uncertainty. Another but compeletely different use of sigmoids is for numerical continuation, i.e. when doing bifurcation analysis with respect to some parameter in the model. Numerical continuation is easier with smooth systems.\n",
        "\n",
        "**3.\tSingle layer vs multi-layer perceptron :**\n",
        "Perceptrons are two-layer networks with one input and one output. Multilayered Networks have at least one hidden layer i.e., all the layers between the input and output layers are hidden. A single-layer perceptron can only learn linear functions, but Multilayered Perceptrons can also learn non-linear functions."
      ],
      "metadata": {
        "id": "wtH2B7wkSjAZ"
      }
    }
  ]
}